{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5fb520c-d210-4786-b135-fa6bf9313f6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/studio-lab-user/sagemaker-studiolab-notebooks/AI-OT24/Reinforcement-Learning-Stock-Porfolio-Managment\")\n",
    "!pip install -q -r requirements.txt\n",
    "import sys\n",
    "sys.path.append('./utils')\n",
    "from trading_functions import *\n",
    "# Core Libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import warnings\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # 0 = all messages, 1 = info, 2 = warnings, 3 = errors\n",
    "\n",
    "# Data Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Financial Data\n",
    "import yfinance as yf\n",
    "import quantstats as qs\n",
    "import ta\n",
    "\n",
    "# Machine Learning - Supervised Learning\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Machine Learning - Deep Learning\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "# Reinforcement Learning and Environments\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import A2C, DDPG, DQN, HER, PPO, SAC, TD3\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import (\n",
    "    EvalCallback, StopTrainingOnRewardThreshold, StopTrainingOnNoModelImprovement\n",
    ")\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv, VecCheckNan\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from sb3_contrib import ARS, MaskablePPO, RecurrentPPO, QRDQN, TRPO\n",
    "\n",
    "# Imitation Learning\n",
    "from imitation.algorithms import bc\n",
    "from imitation.testing.reward_improvement import is_significant_reward_improvement\n",
    "from imitation.data.types import Transitions\n",
    "\n",
    "# Interactive Brokers API\n",
    "from ib_insync import *\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "clear_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c5ba8c-194d-4fee-ad7e-0234638b1754",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\">  19%</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">18,990/100,000 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:00:17</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">0:01:15</span> , <span style=\"color: #800000; text-decoration-color: #800000\">1,087 it/s</span> ]\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[35m  19%\u001b[0m \u001b[38;2;249;38;114m━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18,990/100,000 \u001b[0m [ \u001b[33m0:00:17\u001b[0m < \u001b[36m0:01:15\u001b[0m , \u001b[31m1,087 it/s\u001b[0m ]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define constants\n",
    "SEED = 1\n",
    "history_length = 5\n",
    "reward_type = 'LNR'\n",
    "stocks = ['AAPL', 'AMZN', 'META', 'MSFT', 'NVDA', 'TSLA']\n",
    "start_date = '2020-01-01'\n",
    "end_date = '2023-06-30'\n",
    "date_splits = 4\n",
    "n_envs = 10\n",
    "n_steps = 8\n",
    "total_timesteps = 10_000\n",
    "batch_size = 16\n",
    "learning_rate = 0.001\n",
    "ent_coef = 0.10\n",
    "log_interval = 1_000\n",
    "eval_freq=1_000\n",
    "\n",
    "current_date = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "model_save_dir = f'./models/model_{current_date}'\n",
    "log_dir = f'./logs/log_{current_date}'\n",
    "\n",
    "# Training environment\n",
    "env, vec_env = create_training_env(history_length, reward_type, '2020-01-01', '2023-06-30', stocks, n_envs)\n",
    "\n",
    "# Validation environment\n",
    "valid_env, vec_valid_env = create_evaluation_env(history_length, reward_type, '2023-07-01', '2023-12-30', stocks)\n",
    "\n",
    "# Test environment\n",
    "test_env, _ = create_evaluation_env(history_length, reward_type, '2024-01-01', '2024-10-30', stocks, n_envs=1)\n",
    "\n",
    "# Evaluation callback for saving the best model\n",
    "eval_callback = EvalCallback(\n",
    "    vec_valid_env,\n",
    "    n_eval_episodes=1,\n",
    "    eval_freq=eval_freq,\n",
    "    deterministic=True,\n",
    "    verbose=1,\n",
    "    best_model_save_path=model_save_dir,\n",
    ")\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch as th\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Custom actor (pi) and value function (vf) networks\n",
    "# of two layers of size 32 each with Relu activation function\n",
    "# Note: an extra linear layer will be added on top of the pi and the vf nets, respectively\n",
    "policy_kwargs = dict(activation_fn=th.nn.ReLU,\n",
    "                     net_arch=dict(pi=[64, 64], vf=[64, 64]))\n",
    "\n",
    "model = PPO(\"MlpPolicy\", vec_env, device=\"cpu\", tensorboard_log = log_dir, learning_rate = 0.02, policy_kwargs=policy_kwargs)\n",
    "model.learn(total_timesteps=100_000, callback = eval_callback, log_interval=10, tb_log_name='PPO', reset_num_timesteps=True, progress_bar=True)\n",
    "vec_env.close()\n",
    "date = None\n",
    "date_length = env.df_unscaled.shape[0]\n",
    "for i in range(date_splits):\n",
    "    date_length = date_length//2\n",
    "    print(date_length,date)\n",
    "    date = env.df_unscaled.index[-date_length]\n",
    "    _, vec_env = create_training_env(history_length, reward_type, date, end_date, stocks, n_envs)\n",
    "    model.set_env(vec_env)\n",
    "    param = model.get_parameters()\n",
    "    param['policy.optimizer']['param_groups'][0]['lr'] = param['policy.optimizer']['param_groups'][0]['lr']/2\n",
    "    model.set_parameters(param)\n",
    "    model.learn(total_timesteps=100_000, callback = eval_callback, log_interval=10, tb_log_name='PPO', reset_num_timesteps=False, progress_bar=True)\n",
    "    vec_env.close()\n",
    "    \n",
    "# Evaluate the model\n",
    "evaluate_model(env, model, 'Train', SEED, has_policy=False)\n",
    "evaluate_model(valid_env, model, 'Valid', SEED, has_policy=False)\n",
    "evaluate_model(test_env, model, 'Test', SEED, has_policy=False)\n",
    "\n",
    "\n",
    "# Close environments\n",
    "env.close()\n",
    "valid_env.close()\n",
    "test_env.close()\n",
    "vec_valid_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e220d59b-62fc-4f3d-b592-95ae19336f1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Evaluation callback for saving the best model\\neval_callback = EvalCallback(\\n    vec_valid_env,\\n    n_eval_episodes=1,\\n    eval_freq=eval_freq,\\n    deterministic=True,\\n    verbose=1,\\n    best_model_save_path=model_save_dir,\\n)\\nstop_train_callback = StopTrainingOnNoModelImprovement(max_no_improvement_evals=3, min_evals=5, verbose=1)\\n\\n\\n# Train the model\\nmodel = train_model(\\n    model_name=\"PPO\",  # Can be \\'PPO\\', \\'A2C\\', or \\'DQN\\'\\n    create_model=True,\\n    vec_env=vec_env,\\n    iterations=1,\\n    train_timesteps=total_timesteps,\\n    log_frec=log_interval,\\n    log_base_dir=log_dir,\\n    n_steps=n_steps,\\n    batch_size=batch_size,\\n    learning_rate=learning_rate,\\n    ent_coef=ent_coef\\n)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "# Evaluation callback for saving the best model\n",
    "eval_callback = EvalCallback(\n",
    "    vec_valid_env,\n",
    "    n_eval_episodes=1,\n",
    "    eval_freq=eval_freq,\n",
    "    deterministic=True,\n",
    "    verbose=1,\n",
    "    best_model_save_path=model_save_dir,\n",
    ")\n",
    "stop_train_callback = StopTrainingOnNoModelImprovement(max_no_improvement_evals=3, min_evals=5, verbose=1)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model = train_model(\n",
    "    model_name=\"PPO\",  # Can be 'PPO', 'A2C', or 'DQN'\n",
    "    create_model=True,\n",
    "    vec_env=vec_env,\n",
    "    iterations=1,\n",
    "    train_timesteps=total_timesteps,\n",
    "    log_frec=log_interval,\n",
    "    log_base_dir=log_dir,\n",
    "    n_steps=n_steps,\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    ent_coef=ent_coef\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6acbb832-9a99-42a8-b9e4-e45612280ebc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0003\n",
      "0.00015\n",
      "0.00015\n"
     ]
    }
   ],
   "source": [
    "param = model.get_parameters()\n",
    "print(param['policy.optimizer']['param_groups'][0]['lr'])\n",
    "param['policy.optimizer']['param_groups'][0]['lr'] = param['policy.optimizer']['param_groups'][0]['lr']/2\n",
    "print(param['policy.optimizer']['param_groups'][0]['lr'])\n",
    "model.set_parameters(param)\n",
    "param = model.get_parameters()\n",
    "print(param['policy.optimizer']['param_groups'][0]['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567d16cb-d44a-42d5-af62-e4a11b752ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sagemaker-distribution:Python",
   "language": "python",
   "name": "conda-env-sagemaker-distribution-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
