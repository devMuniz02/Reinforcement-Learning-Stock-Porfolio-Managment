{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7_RPaz91_Rup",
    "outputId": "77fdd5bd-0337-4be9-ebbc-bb99577b178b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#test\n",
    "#(studiolab) studio-lab-user@default:~/sagemaker-studiolab-notebooks/AI-OT24$ tensorboard --logdir logs/\n",
    "#!pip install -q ib_insync numpy pandas yfinance matplotlib ta seaborn gymnasium stable-baselines3 sb3-contrib tensorflow scikit-learn quantstats imitation fredapi typing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install requirements and import all functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/studio-lab-user/sagemaker-studiolab-notebooks/AI-OT24/Reinforcement-Learning-Stock-Porfolio-Managment\")\n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 18:26:37.710684: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions in the 'trading_functions' library:\n",
      "HER\n",
      "accuracy_score\n",
      "bin_reward_func\n",
      "calculate_accuracy\n",
      "check_env\n",
      "clear_output\n",
      "collect_expert_data\n",
      "create_env\n",
      "create_env_unique\n",
      "create_evaluation_env\n",
      "create_training_env\n",
      "evaluate_all\n",
      "evaluate_best\n",
      "evaluate_buy\n",
      "evaluate_model\n",
      "evaluate_policy\n",
      "evaluate_various\n",
      "is_significant_reward_improvement\n",
      "linear_schedule\n",
      "lnr_reward_func\n",
      "make_vec_env\n",
      "save_model\n",
      "smp_reward_func\n",
      "sqh_reward_func\n",
      "sqs_reward_func\n",
      "stp_reward_func\n",
      "train_model\n",
      "train_test_split\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./utils')\n",
    "import trading_functions  # Import your library\n",
    "import inspect  # Used to inspect the module\n",
    "\n",
    "# Get all functions from the library\n",
    "all_functions = [func for func, obj in inspect.getmembers(trading_functions) if inspect.isfunction(obj)]\n",
    "\n",
    "# Print the function names\n",
    "print(\"Functions in the 'trading_functions' library:\")\n",
    "for function in all_functions:\n",
    "    print(function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./utils')\n",
    "from trading_functions import *\n",
    "#from trading_functions import linear_schedule, TradingEnvUnique, TradingEnvUniqueMultiple, create_env, create_env_unique, evaluate_best, evaluate_buy, evaluate_model, evaluate_various, evaluate_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dvsZhCEz-3nT",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Core Libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import warnings\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # 0 = all messages, 1 = info, 2 = warnings, 3 = errors\n",
    "\n",
    "# Data Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Financial Data\n",
    "import yfinance as yf\n",
    "import quantstats as qs\n",
    "import ta\n",
    "\n",
    "# Machine Learning - Supervised Learning\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Machine Learning - Deep Learning\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "# Reinforcement Learning and Environments\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import A2C, DDPG, DQN, HER, PPO, SAC, TD3\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import (\n",
    "    EvalCallback, StopTrainingOnRewardThreshold, StopTrainingOnNoModelImprovement\n",
    ")\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv, VecCheckNan\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from sb3_contrib import ARS, MaskablePPO, RecurrentPPO, QRDQN, TRPO\n",
    "\n",
    "# Imitation Learning\n",
    "from imitation.algorithms import bc\n",
    "from imitation.testing.reward_improvement import is_significant_reward_improvement\n",
    "from imitation.data.types import Transitions\n",
    "\n",
    "# Interactive Brokers API\n",
    "from ib_insync import *\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "clear_output()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API Fred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fredapi import Fred\n",
    "fred = Fred(api_key='xxxxxxxxxxxxxxx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mw_gfycyqrWx",
    "outputId": "a250f37b-2ed8-42d3-bf51-f8344ad4f458",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define constants\n",
    "SEED = 1\n",
    "history_length = 5\n",
    "reward_type = 'SQS'\n",
    "stocks = ['AAPL', 'AMZN', 'META', 'MSFT', 'NVDA', 'TSLA']\n",
    "n_envs = 10\n",
    "n_steps = 8\n",
    "total_timesteps = 10_000\n",
    "batch_size = 16\n",
    "learning_rate = 0.001\n",
    "ent_coef = 0.10\n",
    "log_interval = 1_000\n",
    "eval_freq=1_000\n",
    "\n",
    "current_date = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "model_save_dir = f'./models/model_{current_date}'\n",
    "log_dir = f'./logs/log_{current_date}'\n",
    "\n",
    "# Training environment\n",
    "env, vec_env = create_training_env(history_length, reward_type, '2020-01-01', '2023-06-30', stocks, n_envs)\n",
    "\n",
    "# Validation environment\n",
    "valid_env, vec_valid_env = create_evaluation_env(history_length, reward_type, '2023-07-01', '2023-12-30', stocks)\n",
    "\n",
    "# Test environment\n",
    "test_env, _ = create_evaluation_env(history_length, reward_type, '2024-01-01', '2024-10-30', stocks, n_envs=1)\n",
    "\n",
    "# Evaluation callback for saving the best model\n",
    "eval_callback = EvalCallback(\n",
    "    vec_valid_env,\n",
    "    n_eval_episodes=1,\n",
    "    eval_freq=eval_freq,\n",
    "    deterministic=True,\n",
    "    verbose=1,\n",
    "    best_model_save_path=model_save_dir,\n",
    ")\n",
    "stop_train_callback = StopTrainingOnNoModelImprovement(max_no_improvement_evals=3, min_evals=5, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "model = train_model(\n",
    "    model_name=\"PPO\",  # Can be 'PPO', 'A2C', or 'DQN'\n",
    "    create_model=True,\n",
    "    vec_env=vec_env,\n",
    "    iterations=1,\n",
    "    train_timesteps=total_timesteps,\n",
    "    log_frec=log_interval,\n",
    "    log_base_dir=log_dir,\n",
    "    n_steps=n_steps,\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    ent_coef=ent_coef\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(env, model, 'Train', SEED, has_policy=False)\n",
    "evaluate_model(valid_env, model, 'Valid', SEED, has_policy=False)\n",
    "evaluate_model(test_env, model, 'Test', SEED, has_policy=False)\n",
    "\n",
    "# Save the model to the Tests folder\n",
    "save_model(model, model_save_dir)\n",
    "\n",
    "# Close environments\n",
    "env.close()\n",
    "valid_env.close()\n",
    "test_env.close()\n",
    "vec_env.close()\n",
    "vec_valid_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.df_unscaled to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.df_unscaled` for environment variables or `env.get_wrapper_attr('df_unscaled')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.steps to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.steps` for environment variables or `env.get_wrapper_attr('steps')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.history_length to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.history_length` for environment variables or `env.get_wrapper_attr('history_length')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_stocks to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_stocks` for environment variables or `env.get_wrapper_attr('num_stocks')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.buy_com to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.buy_com` for environment variables or `env.get_wrapper_attr('buy_com')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.sell_com to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.sell_com` for environment variables or `env.get_wrapper_attr('sell_com')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Monitor.reset() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m valid_env, vec_valid_env \u001b[38;5;241m=\u001b[39m create_training_env(history_length, reward_type, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2023-07-01\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2023-08-30\u001b[39m\u001b[38;5;124m'\u001b[39m, stocks, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     20\u001b[0m transitions, expert_actions \u001b[38;5;241m=\u001b[39m collect_expert_data(env, seed\u001b[38;5;241m=\u001b[39mSEED)\n\u001b[0;32m---> 21\u001b[0m bc_trainer \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransitions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransitions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbc_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbc_log_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1_000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m118\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     30\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m model_actions, returns, total_reward \u001b[38;5;241m=\u001b[39m evaluate_model(env, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBC\u001b[39m\u001b[38;5;124m\"\u001b[39m,has_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,SEED\u001b[38;5;241m=\u001b[39mSEED,trainer\u001b[38;5;241m=\u001b[39mbc_trainer)\n\u001b[1;32m     32\u001b[0m action_counts \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(model_actions)\u001b[38;5;241m.\u001b[39mvalue_counts()\n",
      "File \u001b[0;32m~/sagemaker-studiolab-notebooks/AI-OT24/Reinforcement-Learning-Stock-Porfolio-Managment/./utils/trading_functions.py:1031\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model_name, model, create_model, vec_env, env, transitions, iterations, train_timesteps, log_frec, log_base_dir, n_steps, batch_size, learning_rate, ent_coef, seed, bc_batches, bc_log_interval)\u001b[0m\n\u001b[1;32m   1022\u001b[0m             model\u001b[38;5;241m.\u001b[39mlearn(\n\u001b[1;32m   1023\u001b[0m                 total_timesteps\u001b[38;5;241m=\u001b[39mtrain_timesteps,\n\u001b[1;32m   1024\u001b[0m                 progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1027\u001b[0m                 reset_num_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m             )\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBC\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   1030\u001b[0m     \u001b[38;5;66;03m# Evaluate BC policy before training\u001b[39;00m\n\u001b[0;32m-> 1031\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1032\u001b[0m     mean_reward_bc_before, std_reward_bc_before \u001b[38;5;241m=\u001b[39m evaluate_policy(\n\u001b[1;32m   1033\u001b[0m         model\u001b[38;5;241m.\u001b[39mpolicy, env, n_eval_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, return_episode_rewards\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m     )\n\u001b[1;32m   1035\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBC Learner rewards before training:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Monitor.reset() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "# Define constants\n",
    "SEED = 1\n",
    "history_length = 5\n",
    "reward_type = 'LNR'\n",
    "stocks = ['AAPL', 'AMZN', 'META', 'MSFT', 'NVDA', 'TSLA']\n",
    "n_envs = 1\n",
    "n_steps = 8\n",
    "total_timesteps = 10_000\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "ent_coef = 0.10\n",
    "log_interval = 10_000\n",
    "eval_freq=1_000\n",
    "log_dir = './logs'\n",
    "model_save_dir = './models'\n",
    "\n",
    "# Training environment\n",
    "env, vec_env = create_training_env(history_length, reward_type, '2020-01-01', '2023-06-30', stocks, n_envs)\n",
    "valid_env, vec_valid_env = create_training_env(history_length, reward_type, '2023-07-01', '2023-08-30', stocks, 1)\n",
    "transitions, expert_actions = collect_expert_data(env, seed=SEED)\n",
    "bc_trainer = train_model(\n",
    "    model_name=\"BC\",\n",
    "    create_model=True,\n",
    "    env=env,\n",
    "    transitions=transitions,\n",
    "    bc_batches=2000,\n",
    "    bc_log_interval=1_000,\n",
    "    batch_size=118,\n",
    "    seed=1\n",
    ")\n",
    "model_actions, returns, total_reward = evaluate_model(env, name=\"BC\",has_policy=True,SEED=SEED,trainer=bc_trainer)\n",
    "action_counts = pd.Series(model_actions).value_counts()\n",
    "print(\"Action Counts:\")\n",
    "print(action_counts)\n",
    "model_actions, returns, total_reward = evaluate_model(valid_env, name=\"BC\",has_policy=True,SEED=SEED,trainer=bc_trainer)\n",
    "action_counts = pd.Series(model_actions).value_counts()\n",
    "print(\"Action Counts:\")\n",
    "print(action_counts)\n",
    "best_actions, best_returns, total_reward = evaluate_best(env, expert_actions, SEED)\n",
    "action_counts = pd.Series(best_actions).value_counts()\n",
    "print(\"Action Counts:\")\n",
    "print(action_counts)\n",
    "\n",
    "# Close environments\n",
    "env.close()\n",
    "valid_env.close()\n",
    "vec_env.close()\n",
    "vec_valid_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "sagemaker-distribution:Python",
   "language": "python",
   "name": "conda-env-sagemaker-distribution-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
