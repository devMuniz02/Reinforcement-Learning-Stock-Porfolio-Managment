{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7eab85d-9282-410e-98ef-0ac9ba8e89dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/studio-lab-user/sagemaker-studiolab-notebooks/AI-OT-24/Reinforcement-Learning-Stock-Porfolio-Managment\")\n",
    "!pip install -q -r requirements.txt\n",
    "import sys\n",
    "sys.path.append('./utils')\n",
    "from trading_functions import *\n",
    "# Core Libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import warnings\n",
    "import itertools\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # 0 = all messages, 1 = info, 2 = warnings, 3 = errors\n",
    "\n",
    "# Data Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Financial Data\n",
    "import yfinance as yf\n",
    "import quantstats as qs\n",
    "import ta\n",
    "\n",
    "# Machine Learning - Supervised Learning\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Machine Learning - Deep Learning\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "# Reinforcement Learning and Environments\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import A2C, DDPG, DQN, HER, PPO, SAC, TD3\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import (\n",
    "    EvalCallback, StopTrainingOnRewardThreshold, StopTrainingOnNoModelImprovement\n",
    ")\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv, VecCheckNan, VecNormalize\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from sb3_contrib import ARS, MaskablePPO, RecurrentPPO, QRDQN, TRPO\n",
    "\n",
    "# Imitation Learning\n",
    "from imitation.algorithms import bc\n",
    "from imitation.testing.reward_improvement import is_significant_reward_improvement\n",
    "from imitation.data.types import Transitions\n",
    "\n",
    "# Interactive Brokers API\n",
    "from ib_insync import *\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "clear_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a2744c5-824e-49c4-aabe-cadc53d52d71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define constants\n",
    "SEED = 1\n",
    "history_length = [1, 2, 4, 5, 6, 8, 10, 16, 20]\n",
    "reward_type = 'LNR'\n",
    "stocks = ['AAPL', 'AMZN', 'META', 'MSFT', 'NVDA', 'TSLA']\n",
    "start_date = '2015-01-01'\n",
    "end_date = '2023-06-30'\n",
    "n_envs = 8\n",
    "n_steps = 16\n",
    "total_timesteps = 100_000\n",
    "batch_size = 32\n",
    "learning_rate = 0.0001\n",
    "ent_coef = 0.05\n",
    "log_interval = 1_000\n",
    "eval_freq = 1_000\n",
    "model_name= 'A2C'\n",
    "\n",
    "model_save_dir = f'./models/history_length/{history_length}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21da0ae0-5e55-4135-97ca-bdd47a25ebd3",
   "metadata": {},
   "source": [
    "Stage 1: Set time intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e52826e-f2fa-4170-887c-087956c58b3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['AAPL']: OperationalError('unable to open database file')\n",
      "\n",
      "6 Failed downloads:\n",
      "['TSLA', 'META', 'MSFT', 'NVDA', 'AMZN', 'AAPL']: OperationalError('unable to open database file')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "attempt to get argmax of an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Training environment\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_training_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhistory_length\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_envs\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      4\u001b[0m envs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m vec_envs \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/sagemaker-studiolab-notebooks/AI-OT-24/Reinforcement-Learning-Stock-Porfolio-Managment/./utils/trading_functions.py:878\u001b[0m, in \u001b[0;36mcreate_training_env\u001b[0;34m(history_length, reward_type, start_date, end_date, stocks, n_envs)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_training_env\u001b[39m(history_length, reward_type, start_date, end_date, stocks, n_envs):\n\u001b[1;32m    875\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;124;03m    Create a vectorized environment for training.\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m     env, env_fn, date_interval, scalers, df, df_unscaled \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_env\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m     check_env(env)\n\u001b[1;32m    882\u001b[0m     vec_env \u001b[38;5;241m=\u001b[39m make_vec_env(env_fn, n_envs\u001b[38;5;241m=\u001b[39mn_envs, vec_env_cls\u001b[38;5;241m=\u001b[39mSubprocVecEnv)\n",
      "File \u001b[0;32m~/sagemaker-studiolab-notebooks/AI-OT-24/Reinforcement-Learning-Stock-Porfolio-Managment/./utils/trading_functions.py:725\u001b[0m, in \u001b[0;36mcreate_env\u001b[0;34m(history_length, reward_type, start_date, end_date, stocks, scaler, scalers)\u001b[0m\n\u001b[1;32m    723\u001b[0m data[(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m15MA\u001b[39m\u001b[38;5;124m'\u001b[39m, ticker)] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m][ticker]\u001b[38;5;241m.\u001b[39mrolling(window\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    724\u001b[0m data[(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m20MA\u001b[39m\u001b[38;5;124m'\u001b[39m, ticker)] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m][ticker]\u001b[38;5;241m.\u001b[39mrolling(window\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m--> 725\u001b[0m data[(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDaily_Return\u001b[39m\u001b[38;5;124m'\u001b[39m, ticker)] \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mClose\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mticker\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpct_change\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# Adding technical indicators\u001b[39;00m\n\u001b[1;32m    728\u001b[0m bollinger \u001b[38;5;241m=\u001b[39m ta\u001b[38;5;241m.\u001b[39mvolatility\u001b[38;5;241m.\u001b[39mBollingerBands(close\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m][ticker])\n",
      "File \u001b[0;32m/opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages/pandas/core/generic.py:11712\u001b[0m, in \u001b[0;36mNDFrame.pct_change\u001b[0;34m(self, periods, fill_method, limit, freq, **kwargs)\u001b[0m\n\u001b[1;32m  11710\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, col \u001b[38;5;129;01min\u001b[39;00m cols:\n\u001b[1;32m  11711\u001b[0m     mask \u001b[38;5;241m=\u001b[39m col\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m> 11712\u001b[0m     mask \u001b[38;5;241m=\u001b[39m mask[\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m~\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m :]\n\u001b[1;32m  11713\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m  11714\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m  11715\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe default fill_method=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpad\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m  11716\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pct_change is deprecated and will \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  11721\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m  11722\u001b[0m         )\n",
      "File \u001b[0;32m/opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages/numpy/core/fromnumeric.py:1229\u001b[0m, in \u001b[0;36margmax\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;124;03mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[1;32m   1144\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;124;03m(2, 1, 4)\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m kwds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m-> 1229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43margmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/sagemaker-distribution/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[0;31mValueError\u001b[0m: attempt to get argmax of an empty sequence"
     ]
    }
   ],
   "source": [
    "# Training environment\n",
    "env = create_training_env(history_length[-1], reward_type, start_date, end_date, stocks, n_envs)[0]\n",
    "\n",
    "envs = []\n",
    "vec_envs = []\n",
    "models = []\n",
    "\n",
    "# Compute the differences with the next number\n",
    "differences = [history_length[i + 1] - history_length[i] for i in range(len(history_length) - 1)]\n",
    "# Append a placeholder for the last element\n",
    "differences.append(0)  # Placeholder value\n",
    "history_length.reverse()\n",
    "differences.reverse()\n",
    "\n",
    "results = []\n",
    "date_length = env.df_unscaled.shape[0]  # Total number of rows\n",
    "date = env.df_unscaled.index[-date_length]  # Initial start date\n",
    "\n",
    "for j in range(6):  # Loop for 6 time splits\n",
    "    if j != 0:\n",
    "        date_length = date_length // 2  # Halve the date length each iteration\n",
    "    date = env.df_unscaled.index[-date_length]  # Update the start date for the split\n",
    "    \n",
    "    for i in range(len(history_length)):  # Iterate over history lengths\n",
    "        start_date = env.df_unscaled.index[-date_length + sum(differences[:i + 1])]\n",
    "        \n",
    "        # Save the results as dictionaries in a list\n",
    "        results.append({\n",
    "            'Timesplit': j,\n",
    "            'Historylength': history_length[i],\n",
    "            'Startdate': start_date\n",
    "        })\n",
    "# Convert results into a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Accessing the specific record in the DataFrame\n",
    "timesplit_value = 5\n",
    "history_length_value = 10\n",
    "\n",
    "# Filter the DataFrame for the specified conditions\n",
    "filtered_result = results_df[(results_df['Timesplit'] == timesplit_value) & \n",
    "                             (results_df['Historylength'] == history_length_value)]\n",
    "\n",
    "# Extract the start date\n",
    "start_date_result = filtered_result['Startdate'].iloc[0] if not filtered_result.empty else None\n",
    "\n",
    "start_date_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba076a68-e2c3-45fd-95e7-fe5e53e139f5",
   "metadata": {},
   "source": [
    "Stage 1: Multiple stock learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2478171-7451-466e-9109-1de2ce213d7f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Example list of 18 stock tickers\n",
    "stock_list = ['NVDA', 'TSLA', 'PLTR', 'AVGO', 'F', 'INTC', 'SMCI', 'MU', 'AAPL', 'AMD', 'PFE', 'T', 'UBER', 'AMZN', 'GOOGL', 'WBD', 'BAC', 'WBA', 'GOOG', 'CVS', 'MSFT', 'CSCO', 'CMCSA', 'WFC', 'KO', 'WMT', 'VZ', 'CCL', 'XOM', 'HBAN', 'AMCR', 'HPE', 'OXY', 'SLB', 'LRCX', 'DVN', 'BA', 'NKE', 'C', 'KDP', 'KHC', 'BMY', 'NEM', 'META', 'KMI', 'PCG', 'CSX', 'AES', 'FCX', 'SCHW', 'GM', 'MRK', 'CVX', 'KEY', 'KVUE', 'MDLZ', 'SBUX', 'DIS', 'HAL', 'UNH', 'MCHP', 'NEE', 'DOW', 'JPM', 'PYPL', 'QCOM', 'EXC', 'JNJ', 'PARA', 'ORCL', 'TFC', 'APH', 'CNC', 'COP', 'ANET', 'USB', 'RF', 'MO', 'HPQ', 'CMG', 'NCLH', 'HST', 'VTRS', 'AMAT', 'WMB', 'ON', 'APA', 'V', 'ADBE', 'WDC', 'GILD', 'EQT', 'MRNA', 'DAL', 'DELL', 'PEP', 'CRM', 'CARR', 'LW', 'MS', 'ABBV', 'EW', 'GIS', 'IPG', 'LUV', 'CTRA', 'GE', 'PG', 'CCI', 'TJX', 'CNP', 'MDT']\n",
    "\n",
    "# Split the list into groups of 6 stocks without repetition\n",
    "n_groups = len(stock_list) // history_length  # This should be 3 for 18/6\n",
    "groups = [stock_list[i * history_length:(i + 1) * history_length] for i in range(n_groups)]\n",
    "\n",
    "# Create environments for each group of stocks\n",
    "envs = []\n",
    "vec_env = None\n",
    "\n",
    "# Validation environment\n",
    "valid_env, vec_valid_env = create_evaluation_env(history_length, reward_type, '2023-07-01', '2023-12-30', ['TGT', 'MARA', 'GOOGL', 'WMT', 'V', 'PG'])\n",
    "\n",
    "# Evaluation callback for saving the best model\n",
    "eval_callback = EvalCallback(\n",
    "    vec_valid_env,\n",
    "    n_eval_episodes=1,\n",
    "    eval_freq=eval_freq,\n",
    "    deterministic=True,\n",
    "    verbose=0,\n",
    "    best_model_save_path=model_save_dir,\n",
    ")\n",
    "\n",
    "_, vec_env = create_training_env(history_length, reward_type, start_date, end_date, groups[0], n_envs)\n",
    "\n",
    "model = None\n",
    "    try:\n",
    "        model = A2C.load(model_save_dir)\n",
    "    except:\n",
    "        model = A2C('MlpPolicy', \n",
    "                        vec_env,\n",
    "                        learning_rate=0.002,\n",
    "                        n_steps=8,\n",
    "                        gamma=0.99,\n",
    "                        gae_lambda=1.0,\n",
    "                        ent_coef=0.05,\n",
    "                        vf_coef=0.5,\n",
    "                        max_grad_norm=0.5,\n",
    "                        rms_prop_eps=1e-05,\n",
    "                        use_rms_prop=True,\n",
    "                        use_sde=False,\n",
    "                        sde_sample_freq=-1,\n",
    "                        rollout_buffer_class=None,\n",
    "                        rollout_buffer_kwargs=None,\n",
    "                        normalize_advantage=False,\n",
    "                        stats_window_size=100,\n",
    "                        tensorboard_log=log_dir,\n",
    "                        policy_kwargs=None,\n",
    "                        verbose=0,\n",
    "                        seed=0,\n",
    "                        device='auto',\n",
    "                        _init_setup_model=True)\n",
    "\n",
    "for stocks in groups:\n",
    "    env, vec_env = create_training_env(history_length, reward_type, start_date, end_date, stocks, n_envs)\n",
    "    envs.append(env)\n",
    "    model.set_env(vec_env)\n",
    "    model.learn(\n",
    "            total_timesteps=total_timesteps,\n",
    "            progress_bar=False,\n",
    "            log_interval=log_interval,\n",
    "            tb_log_name=f\"A2C\",\n",
    "            reset_num_timesteps=False,\n",
    "            callback=eval_callback\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f0148e-1f90-4fdb-9e8c-07153a7c510b",
   "metadata": {},
   "source": [
    "Stage 2: Multiple time learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c38071-db1e-46ea-87b1-8b661e14f700",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "date = None\n",
    "date_length = env.df_unscaled.shape[0]\n",
    "for i in range(4):\n",
    "    date_length = date_length//2\n",
    "    print(date_length,date)\n",
    "    date = env.df_unscaled.index[-date_length]\n",
    "    _, vec_env = create_training_env(history_length, reward_type, date, end_date, stocks, n_envs)\n",
    "    model.set_env(vec_env)\n",
    "    param = model.get_parameters()\n",
    "    param['policy.optimizer']['param_groups'][0]['lr'] = param['policy.optimizer']['param_groups'][0]['lr']/2\n",
    "    model.set_parameters(param)\n",
    "    model.learn(\n",
    "            total_timesteps=total_timesteps,\n",
    "            progress_bar=False,\n",
    "            log_interval=log_interval,\n",
    "            tb_log_name=f\"A2C\",\n",
    "            reset_num_timesteps=False,\n",
    "            callback=eval_callback\n",
    "        )\n",
    "    vec_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1243e1-a889-4cb0-a5a1-e05e8681865c",
   "metadata": {},
   "source": [
    "Stage 3: Multiple history length models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eeab8a-afbf-47d6-b456-01e0b63348da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training environment\n",
    "env = create_training_env(history_length[-1], reward_type, start_date, end_date, stocks, n_envs)[0]\n",
    "\n",
    "# Test environment\n",
    "test_env, _ = create_evaluation_env(history_length[-1], reward_type, '2023-07-01', '2024-12-01', stocks,n_envs=1)\n",
    "\n",
    "envs = []\n",
    "vec_envs = []\n",
    "models = []\n",
    "\n",
    "# Compute the differences with the next number\n",
    "differences = [history_length[i + 1] - history_length[i] for i in range(len(history_length) - 1)]\n",
    "# Append a placeholder for the last element\n",
    "differences.append(0)  # Placeholder value\n",
    "history_length.reverse()\n",
    "differences.reverse()\n",
    "\n",
    "for i in range(len(history_length)): \n",
    "    date = env.df_unscaled.index[differences[i]]\n",
    "    print(differences[i],date)\n",
    "    env, vec_env = create_training_env(history_length[i], reward_type, date, end_date, stocks, n_envs)\n",
    "    envs.append(env)\n",
    "    vec_envs.append(vec_env)\n",
    "\n",
    "    model = A2C('MlpPolicy', \n",
    "                    vec_envs[i],\n",
    "                    learning_rate=0.0001,\n",
    "                    n_steps=16,\n",
    "                    gamma=0.99,\n",
    "                    gae_lambda=1.0,\n",
    "                    ent_coef=0.10,\n",
    "                    vf_coef=0.5,\n",
    "                    max_grad_norm=0.5,\n",
    "                    rms_prop_eps=1e-05,\n",
    "                    use_rms_prop=True,\n",
    "                    use_sde=False,\n",
    "                    sde_sample_freq=-1,\n",
    "                    rollout_buffer_class=None,\n",
    "                    rollout_buffer_kwargs=None,\n",
    "                    normalize_advantage=False,\n",
    "                    stats_window_size=100,\n",
    "                    tensorboard_log=log_dir,\n",
    "                    policy_kwargs=None,\n",
    "                    verbose=0,\n",
    "                    seed=0,\n",
    "                    device='auto',\n",
    "                    _init_setup_model=True)\n",
    "    models.append(model)\n",
    "    models[i].learn(\n",
    "            total_timesteps=total_timesteps,\n",
    "            progress_bar=False,\n",
    "            log_interval=log_interval,\n",
    "            tb_log_name=f\"A2C\",\n",
    "            reset_num_timesteps=False,\n",
    "            #callback=eval_callback\n",
    "        )\n",
    "    \n",
    "    # Evaluate the model on training and test environments\n",
    "    print(f\"Evaluating model: {model_name,i}\")\n",
    "\n",
    "    # Training evaluation\n",
    "    mean_train_reward, std_train_reward = evaluate_policy(\n",
    "        models[i].policy if hasattr(models[i], 'policy') else models[i],\n",
    "        envs[i],\n",
    "        n_eval_episodes=1,\n",
    "        deterministic=True\n",
    "    )\n",
    "    print(f\"{model_name} Train Mean reward: {mean_train_reward:.2f} Â± {std_train_reward:.2f}\")\n",
    "    #evaluate_model(envs[i], models[i], model_name, 1, has_policy=True) if hasattr(models[i], 'policy') else evaluate_model(envs[i], models[i], model_name, 1, has_policy=False)\n",
    "    print('Steps',envs[i].steps)\n",
    "    vec_envs[i].close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sagemaker-distribution:Python",
   "language": "python",
   "name": "conda-env-sagemaker-distribution-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
