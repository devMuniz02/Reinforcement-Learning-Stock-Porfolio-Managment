{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7eab85d-9282-410e-98ef-0ac9ba8e89dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/studio-lab-user/sagemaker-studiolab-notebooks/AI-OT-24/Reinforcement-Learning-Stock-Porfolio-Managment\")\n",
    "!pip install -q -r requirements.txt\n",
    "import sys\n",
    "sys.path.append('./utils')\n",
    "from trading_functions import *\n",
    "# Core Libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import warnings\n",
    "import itertools\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # 0 = all messages, 1 = info, 2 = warnings, 3 = errors\n",
    "\n",
    "# Data Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Financial Data\n",
    "import yfinance as yf\n",
    "import quantstats as qs\n",
    "import ta\n",
    "\n",
    "# Machine Learning - Supervised Learning\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Machine Learning - Deep Learning\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "# Reinforcement Learning and Environments\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import A2C, DDPG, DQN, HER, PPO, SAC, TD3\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import (\n",
    "    EvalCallback, StopTrainingOnRewardThreshold, StopTrainingOnNoModelImprovement\n",
    ")\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv, VecCheckNan, VecNormalize\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from sb3_contrib import ARS, MaskablePPO, RecurrentPPO, QRDQN, TRPO\n",
    "\n",
    "# Imitation Learning\n",
    "from imitation.algorithms import bc\n",
    "from imitation.testing.reward_improvement import is_significant_reward_improvement\n",
    "from imitation.data.types import Transitions\n",
    "\n",
    "# Interactive Brokers API\n",
    "from ib_insync import *\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "clear_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a2744c5-824e-49c4-aabe-cadc53d52d71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define constants\n",
    "SEED = 1\n",
    "history_length = [1, 2, 4, 5, 6, 8, 10, 16, 20]\n",
    "reward_type = 'LNR'\n",
    "stocks = ['AAPL', 'AMZN', 'META', 'MSFT', 'NVDA', 'TSLA']\n",
    "start_date = '2015-01-01'\n",
    "end_date = '2023-06-30'\n",
    "n_envs = 8\n",
    "n_steps = 16\n",
    "total_timesteps = 100_000\n",
    "batch_size = 32\n",
    "learning_rate = 0.0001\n",
    "ent_coef = 0.05\n",
    "log_interval = 1_000\n",
    "eval_freq = 1_000\n",
    "model_name= 'A2C'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21da0ae0-5e55-4135-97ca-bdd47a25ebd3",
   "metadata": {},
   "source": [
    "Stage 1: Set time intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e52826e-f2fa-4170-887c-087956c58b3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training environment\n",
    "env = create_training_env(history_length[-1], reward_type, start_date, end_date, stocks, n_envs)[0]\n",
    "\n",
    "# Compute the differences with the next number\n",
    "differences = [history_length[i + 1] - history_length[i] for i in range(len(history_length) - 1)]\n",
    "# Append a placeholder for the last element\n",
    "differences.append(0)  # Placeholder value\n",
    "history_length.reverse()\n",
    "differences.reverse()\n",
    "\n",
    "results = []\n",
    "# Initialize timesplits as a list of empty lists for each timesplit\n",
    "timesplits = [[] for _ in range(6)]\n",
    "\n",
    "date_length = env.df_unscaled.shape[0]  # Total number of rows\n",
    "date = env.df_unscaled.index[-date_length]  # Initial start date\n",
    "\n",
    "for j in range(6):  # Loop for 6 time splits\n",
    "    if j != 0:\n",
    "        date_length = date_length // 2  # Halve the date length each iteration\n",
    "    date = env.df_unscaled.index[-date_length]  # Update the start date for the split\n",
    "    \n",
    "    for i in range(len(history_length)):  # Iterate over history lengths\n",
    "        start_date = env.df_unscaled.index[-date_length + sum(differences[:i + 1])]\n",
    "        \n",
    "        # Append the start_date to the corresponding timesplit\n",
    "        timesplits[j].append(start_date)\n",
    "timesplits = pd.DataFrame(timesplits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba076a68-e2c3-45fd-95e7-fe5e53e139f5",
   "metadata": {},
   "source": [
    "Stage 1: Multiple stock learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2478171-7451-466e-9109-1de2ce213d7f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Example list of 18 stock tickers\n",
    "stock_list = ['NVDA', 'TSLA', 'PLTR', 'AVGO', 'F', 'INTC', 'SMCI', 'MU', 'AAPL', 'AMD', 'PFE', 'T', 'UBER', 'AMZN', 'GOOGL', 'WBD', 'BAC', 'WBA', 'GOOG', 'CVS', 'MSFT', 'CSCO', 'CMCSA', 'WFC', 'KO', 'WMT', 'VZ', 'CCL', 'XOM', 'HBAN', 'AMCR', 'HPE', 'OXY', 'SLB', 'LRCX', 'DVN', 'BA', 'NKE', 'C', 'KDP', 'KHC', 'BMY', 'NEM', 'META', 'KMI', 'PCG', 'CSX', 'AES', 'FCX', 'SCHW', 'GM', 'MRK', 'CVX', 'KEY', 'KVUE', 'MDLZ', 'SBUX', 'DIS', 'HAL', 'UNH', 'MCHP', 'NEE', 'DOW', 'JPM', 'PYPL', 'QCOM', 'EXC', 'JNJ', 'PARA', 'ORCL', 'TFC', 'APH', 'CNC', 'COP', 'ANET', 'USB', 'RF', 'MO', 'HPQ', 'CMG', 'NCLH', 'HST', 'VTRS', 'AMAT', 'WMB', 'ON', 'APA', 'V', 'ADBE', 'WDC', 'GILD', 'EQT', 'MRNA', 'DAL', 'DELL', 'PEP', 'CRM', 'CARR', 'LW', 'MS', 'ABBV', 'EW', 'GIS', 'IPG', 'LUV', 'CTRA', 'GE', 'PG', 'CCI', 'TJX', 'CNP', 'MDT']\n",
    "\n",
    "history_lengths = history_length\n",
    "history_length = None\n",
    "\n",
    "for h_l, history_length in enumerate(history_lengths):\n",
    "    model_save_dir = f'./models/history_length/{history_length}'\n",
    "    log_dir = f'./logs/history_length/{history_length}'\n",
    "    # Split the list into groups of 6 stocks without repetition\n",
    "    n_groups = len(stock_list) // history_length  # This should be 3 for 18/6\n",
    "    groups = [stock_list[i * history_length:(i + 1) * history_length] for i in range(n_groups)]\n",
    "\n",
    "    # Create environments for each group of stocks\n",
    "    env = None\n",
    "    vec_env = None\n",
    "\n",
    "    # Validation environment\n",
    "    valid_env, vec_valid_env = create_evaluation_env(history_length, reward_type, '2023-07-01', '2023-12-30', ['TGT', 'MARA', 'GOOGL', 'WMT', 'V', 'PG'])\n",
    "\n",
    "    # Evaluation callback for saving the best model\n",
    "    eval_callback = EvalCallback(\n",
    "        vec_valid_env,\n",
    "        n_eval_episodes=1,\n",
    "        eval_freq=eval_freq,\n",
    "        deterministic=True,\n",
    "        verbose=0,\n",
    "        best_model_save_path=model_save_dir,\n",
    "    )\n",
    "\n",
    "    _, vec_env = create_training_env(history_length, reward_type, start_date, end_date, groups[0], n_envs)\n",
    "\n",
    "    model = None\n",
    "    model = A2C('MlpPolicy', \n",
    "                    vec_env,\n",
    "                    learning_rate=0.002,\n",
    "                    n_steps=8,\n",
    "                    gamma=0.99,\n",
    "                    gae_lambda=1.0,\n",
    "                    ent_coef=0.05,\n",
    "                    vf_coef=0.5,\n",
    "                    max_grad_norm=0.5,\n",
    "                    rms_prop_eps=1e-05,\n",
    "                    use_rms_prop=True,\n",
    "                    use_sde=False,\n",
    "                    sde_sample_freq=-1,\n",
    "                    rollout_buffer_class=None,\n",
    "                    rollout_buffer_kwargs=None,\n",
    "                    normalize_advantage=False,\n",
    "                    stats_window_size=100,\n",
    "                    tensorboard_log=log_dir,\n",
    "                    policy_kwargs=None,\n",
    "                    verbose=0,\n",
    "                    seed=0,\n",
    "                    device='auto',\n",
    "                    _init_setup_model=True)\n",
    "\n",
    "    for timesplit in range(timesplits.shape[0]):\n",
    "        date = timesplits[h_l][timesplit]\n",
    "        if timesplit != 0:\n",
    "                param = model.get_parameters()\n",
    "                param['policy.optimizer']['param_groups'][0]['lr'] = param['policy.optimizer']['param_groups'][0]['lr']/2\n",
    "                model.set_parameters(param)\n",
    "        for stocks in groups:\n",
    "            env, vec_env = create_training_env(history_length, reward_type, date, end_date, stocks, n_envs)\n",
    "            model.set_env(vec_env)\n",
    "            model.learn(\n",
    "                    total_timesteps=10_000,\n",
    "                    progress_bar=False,\n",
    "                    log_interval=log_interval,\n",
    "                    tb_log_name=f\"A2C{timesplit}\",\n",
    "                    reset_num_timesteps=False,\n",
    "                    callback=eval_callback\n",
    "                )\n",
    "            model.save(f'{model_save_dir}/{timesplit}')\n",
    "\n",
    "    env.close()\n",
    "    vec_env.close()\n",
    "    valid_env.close()\n",
    "    vec_valid_env.close()\n",
    "    del env \n",
    "    del vec_env \n",
    "    del valid_env \n",
    "    del vec_valid_env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f0148e-1f90-4fdb-9e8c-07153a7c510b",
   "metadata": {},
   "source": [
    "Stage 2: Multiple time learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c38071-db1e-46ea-87b1-8b661e14f700",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "date = None\n",
    "date_length = env.df_unscaled.shape[0]\n",
    "for i in range(4):\n",
    "    date_length = date_length//2\n",
    "    print(date_length,date)\n",
    "    date = env.df_unscaled.index[-date_length]\n",
    "    _, vec_env = create_training_env(history_length, reward_type, date, end_date, stocks, n_envs)\n",
    "    model.set_env(vec_env)\n",
    "    param = model.get_parameters()\n",
    "    param['policy.optimizer']['param_groups'][0]['lr'] = param['policy.optimizer']['param_groups'][0]['lr']/2\n",
    "    model.set_parameters(param)\n",
    "    model.learn(\n",
    "            total_timesteps=total_timesteps,\n",
    "            progress_bar=False,\n",
    "            log_interval=log_interval,\n",
    "            tb_log_name=f\"A2C\",\n",
    "            reset_num_timesteps=False,\n",
    "            callback=eval_callback\n",
    "        )\n",
    "    vec_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1243e1-a889-4cb0-a5a1-e05e8681865c",
   "metadata": {},
   "source": [
    "Stage 3: Multiple history length models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eeab8a-afbf-47d6-b456-01e0b63348da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training environment\n",
    "env = create_training_env(history_length[-1], reward_type, start_date, end_date, stocks, n_envs)[0]\n",
    "\n",
    "# Test environment\n",
    "test_env, _ = create_evaluation_env(history_length[-1], reward_type, '2023-07-01', '2024-12-01', stocks,n_envs=1)\n",
    "\n",
    "envs = []\n",
    "vec_envs = []\n",
    "models = []\n",
    "\n",
    "# Compute the differences with the next number\n",
    "differences = [history_length[i + 1] - history_length[i] for i in range(len(history_length) - 1)]\n",
    "# Append a placeholder for the last element\n",
    "differences.append(0)  # Placeholder value\n",
    "history_length.reverse()\n",
    "differences.reverse()\n",
    "\n",
    "for i in range(len(history_length)): \n",
    "    date = env.df_unscaled.index[differences[i]]\n",
    "    print(differences[i],date)\n",
    "    env, vec_env = create_training_env(history_length[i], reward_type, date, end_date, stocks, n_envs)\n",
    "    envs.append(env)\n",
    "    vec_envs.append(vec_env)\n",
    "\n",
    "    model = A2C('MlpPolicy', \n",
    "                    vec_envs[i],\n",
    "                    learning_rate=0.0001,\n",
    "                    n_steps=16,\n",
    "                    gamma=0.99,\n",
    "                    gae_lambda=1.0,\n",
    "                    ent_coef=0.10,\n",
    "                    vf_coef=0.5,\n",
    "                    max_grad_norm=0.5,\n",
    "                    rms_prop_eps=1e-05,\n",
    "                    use_rms_prop=True,\n",
    "                    use_sde=False,\n",
    "                    sde_sample_freq=-1,\n",
    "                    rollout_buffer_class=None,\n",
    "                    rollout_buffer_kwargs=None,\n",
    "                    normalize_advantage=False,\n",
    "                    stats_window_size=100,\n",
    "                    tensorboard_log=log_dir,\n",
    "                    policy_kwargs=None,\n",
    "                    verbose=0,\n",
    "                    seed=0,\n",
    "                    device='auto',\n",
    "                    _init_setup_model=True)\n",
    "    models.append(model)\n",
    "    models[i].learn(\n",
    "            total_timesteps=total_timesteps,\n",
    "            progress_bar=False,\n",
    "            log_interval=log_interval,\n",
    "            tb_log_name=f\"A2C\",\n",
    "            reset_num_timesteps=False,\n",
    "            #callback=eval_callback\n",
    "        )\n",
    "    \n",
    "    # Evaluate the model on training and test environments\n",
    "    print(f\"Evaluating model: {model_name,i}\")\n",
    "\n",
    "    # Training evaluation\n",
    "    mean_train_reward, std_train_reward = evaluate_policy(\n",
    "        models[i].policy if hasattr(models[i], 'policy') else models[i],\n",
    "        envs[i],\n",
    "        n_eval_episodes=1,\n",
    "        deterministic=True\n",
    "    )\n",
    "    print(f\"{model_name} Train Mean reward: {mean_train_reward:.2f} Â± {std_train_reward:.2f}\")\n",
    "    #evaluate_model(envs[i], models[i], model_name, 1, has_policy=True) if hasattr(models[i], 'policy') else evaluate_model(envs[i], models[i], model_name, 1, has_policy=False)\n",
    "    print('Steps',envs[i].steps)\n",
    "    vec_envs[i].close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sagemaker-distribution:Python",
   "language": "python",
   "name": "conda-env-sagemaker-distribution-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
