{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7eab85d-9282-410e-98ef-0ac9ba8e89dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/studio-lab-user/sagemaker-studiolab-notebooks/AI-OT24/Reinforcement-Learning-Stock-Porfolio-Managment\")\n",
    "!pip install -q -r requirements.txt\n",
    "import sys\n",
    "sys.path.append('./utils')\n",
    "from trading_functions import *\n",
    "# Core Libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import warnings\n",
    "import itertools\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # 0 = all messages, 1 = info, 2 = warnings, 3 = errors\n",
    "\n",
    "# Data Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Financial Data\n",
    "import yfinance as yf\n",
    "import quantstats as qs\n",
    "import ta\n",
    "\n",
    "# Machine Learning - Supervised Learning\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Machine Learning - Deep Learning\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "# Reinforcement Learning and Environments\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import A2C, DDPG, DQN, HER, PPO, SAC, TD3\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import (\n",
    "    EvalCallback, StopTrainingOnRewardThreshold, StopTrainingOnNoModelImprovement\n",
    ")\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv, VecCheckNan, VecNormalize\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from sb3_contrib import ARS, MaskablePPO, RecurrentPPO, QRDQN, TRPO\n",
    "\n",
    "# Imitation Learning\n",
    "from imitation.algorithms import bc\n",
    "from imitation.testing.reward_improvement import is_significant_reward_improvement\n",
    "from imitation.data.types import Transitions\n",
    "\n",
    "# Interactive Brokers API\n",
    "from ib_insync import *\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "clear_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547961e7-ec5e-4e9d-946e-0bb887dcd38a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2023-01-03 00:00:00\n",
      "6 2023-01-11 00:00:00\n",
      "2 2023-01-13 00:00:00\n",
      "1 2023-01-17 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Define constants\n",
    "SEED = 1\n",
    "history_length = [1, 2, 3, 4, 5, 6, 7, 8, 10, 16]\n",
    "reward_type = 'LNR'\n",
    "stocks = ['AAPL', 'AMZN', 'META', 'MSFT', 'NVDA', 'TSLA']\n",
    "start_date = '2023-01-01'\n",
    "end_date = '2023-06-30'\n",
    "n_envs = 8\n",
    "n_steps = 16\n",
    "total_timesteps = 20_000\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "ent_coef = 0.05\n",
    "log_interval = 1_000\n",
    "eval_freq = 1_000\n",
    "model_name= 'A2C'\n",
    "\n",
    "current_date = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "model_save_dir = None #f'./models/model_{current_date}'\n",
    "log_dir = None #f'./logs/log_{current_date}'\n",
    "\n",
    "\n",
    "# Training environment\n",
    "env, vec_env= create_training_env(history_length[-1], reward_type, start_date, end_date, stocks, n_envs)\n",
    "envs = []\n",
    "vec_envs = []\n",
    "envs.append(env)\n",
    "vec_envs.append(vec_env)\n",
    "\n",
    "# Compute the differences with the next number\n",
    "differences = [history_length[i + 1] - history_length[i] for i in range(len(history_length) - 1)]\n",
    "# Append a placeholder for the last element\n",
    "differences.append(0)  # Placeholder value\n",
    "history_length.reverse()\n",
    "differences.reverse()\n",
    "\n",
    "for i in range(len(history_length)): \n",
    "    date = envs[i].df_unscaled.index[differences[i]]\n",
    "    print(differences[i],date)\n",
    "    env, vec_env = create_training_env(history_length[i], reward_type, date, end_date, stocks, n_envs)\n",
    "    envs.append(env)\n",
    "    vec_envs.append(vec_env)\n",
    "    \n",
    "models = []\n",
    "for i in range(len(history_length)):\n",
    "    model = A2C('MlpPolicy', \n",
    "                    vec_envs[i],\n",
    "                    learning_rate=0.002,\n",
    "                    n_steps=8,\n",
    "                    gamma=0.99,\n",
    "                    gae_lambda=1.0,\n",
    "                    ent_coef=0.05,\n",
    "                    vf_coef=0.5,\n",
    "                    max_grad_norm=0.5,\n",
    "                    rms_prop_eps=1e-05,\n",
    "                    use_rms_prop=True,\n",
    "                    use_sde=False,\n",
    "                    sde_sample_freq=-1,\n",
    "                    rollout_buffer_class=None,\n",
    "                    rollout_buffer_kwargs=None,\n",
    "                    normalize_advantage=False,\n",
    "                    stats_window_size=100,\n",
    "                    tensorboard_log=log_dir,\n",
    "                    policy_kwargs=None,\n",
    "                    verbose=0,\n",
    "                    seed=0,\n",
    "                    device='auto',\n",
    "                    _init_setup_model=True)\n",
    "    models.append(model)\n",
    "    models[i].learn(\n",
    "            total_timesteps=total_timesteps,\n",
    "            progress_bar=False,\n",
    "            log_interval=log_interval,\n",
    "            tb_log_name=f\"A2C\",\n",
    "            reset_num_timesteps=False,\n",
    "            #callback=eval_callback\n",
    "        )\n",
    "    \n",
    "    # Evaluate the model on training and test environments\n",
    "    print(f\"Evaluating model: {model_name,i}\")\n",
    "\n",
    "    # Training evaluation\n",
    "    mean_train_reward, std_train_reward = evaluate_policy(\n",
    "        models[i].policy if hasattr(models[i], 'policy') else models[i],\n",
    "        envs[i],\n",
    "        n_eval_episodes=1,\n",
    "        deterministic=True\n",
    "    )\n",
    "    print(f\"{model_name} Train Mean reward: {mean_train_reward:.2f} Â± {std_train_reward:.2f}\")\n",
    "    evaluate_model(envs[i], models[i], model_name, 1, has_policy=True) if hasattr(models[i], 'policy') else evaluate_model(envs[i], models[i], model_name, 1, has_policy=False)\n",
    "    print('Steps',envs[i].steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d705e2e-1ee3-4c7e-933c-0debf8be3624",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Initialize the environment and variables\n",
    "observation, info = envs[0].reset(seed=SEED)\n",
    "total_reward = 0\n",
    "has_policy = False\n",
    "\n",
    "while True:\n",
    "    actions = []\n",
    "    # Get actions from models\n",
    "    if has_policy:\n",
    "        actions.append(int(models[0].policy.predict(observation, deterministic=True)[0]))\n",
    "        for i in range(1,len(history_length)):\n",
    "            actions.append(int(models[i].policy.predict(observation[-(envs[i].ColSize*envs[i].num_stocks*envs[i].history_length):], deterministic=True)[0]))\n",
    "    else:\n",
    "        actions.append(int(models[0].predict(observation, deterministic=True)[0]))\n",
    "        for i in range(1,len(history_length)):\n",
    "            actions.append(int(models[i].predict(observation[-(envs[i].ColSize*envs[i].num_stocks*envs[i].history_length):], deterministic=True)[0]))\n",
    "\n",
    "    # Convert actions to hashable types (e.g., integers or tuples)\n",
    "    action = Counter(actions).most_common(1)[0][0]\n",
    "    print(actions,action)\n",
    "\n",
    "    # Step in the environment\n",
    "    observation, reward, done, _, _ = envs[0].step(action)\n",
    "\n",
    "    # Accumulate total reward\n",
    "    total_reward += reward\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(f\"Total reward: {total_reward}\")\n",
    "envs[0].render('total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f376e476-bd1c-489e-b129-6a9578e559ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sagemaker-distribution:Python",
   "language": "python",
   "name": "conda-env-sagemaker-distribution-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
